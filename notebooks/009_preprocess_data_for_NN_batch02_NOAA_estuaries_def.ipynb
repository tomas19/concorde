{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import geopy.distance\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from scipy import interpolate\n",
    "import matplotlib.pyplot as plt\n",
    "from concorde.tools import get_list, readFort22, from_mag_to_uv\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "ccolors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "pd.options.display.max_rows = 10\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from matplotlib.offsetbox import AnchoredText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathout = Path(r'../models/adcirc/concorde/batch02/_postprocessing/_preprocessForNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dctTracks0 = pd.read_pickle(pathout/'dct_tracksAll_batch02_lengthCorr_tides_resampled.pkl')\n",
    "dctTracks1 = pd.read_pickle(pathout/'dct_tracksAll_batch02_estuaries_lengthCorr_tides_resampled.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols0 = ['lon', 'lat', 'wind_speed', 'pressure', 'rad_to_max_ws', 'heading_dir', 'forward_speed', 'forward_speed_u', 'forward_speed_v']\n",
    "cols1 = [f'{x}_fft' for x in cols0]\n",
    "cols3 = ['Duck', 'Oregon', 'Hatteras', 'Beaufort', 'Wilmington', 'Wrightsville', 'Albemarle', 'Pamlico', 'Neuse']\n",
    "cols21 = [f'dist_to_{x.lower()}' for x in cols3[:6]]\n",
    "cols22 = [f'dist_to_{x}' for x in cols3[6:]]\n",
    "\n",
    "cols = cols0 + cols1 + cols21 + cols22 + cols3 + ['Boundary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "auxcols = ['Albemarle', 'dist_to_Albemarle', 'Pamlico', 'dist_to_Pamlico', 'Neuse', 'dist_to_Neuse']\n",
    "dctTracks = {}\n",
    "for ik, k in enumerate(dctTracks0.keys()):\n",
    "    aux = pd.concat([dctTracks0[k], dctTracks1[k][auxcols]], axis = 1)\n",
    "    aux = aux.loc[:, cols]\n",
    "    dctTracks[k] = aux\n",
    "    # break\n",
    "    if ik == 0:\n",
    "        aux.to_csv(pathout/'dct_tracksAll_batch02_ALL_lengthCorr_tides_resampled_SAMPLE.csv')\n",
    "\n",
    "with open(pathout/'dct_tracksAll_batch02_ALL_lengthCorr_tides_resampled.pkl', 'wb') as fout:\n",
    "    pickle.dump(dctTracks, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>length</th>\n",
       "      <td>1813.0</td>\n",
       "      <td>99.923883</td>\n",
       "      <td>28.306754</td>\n",
       "      <td>28.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>235.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         count       mean        std   min   25%    50%    75%    max\n",
       "length  1813.0  99.923883  28.306754  28.0  79.0  109.0  118.0  235.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths = []\n",
    "for k in sorted(dctTracks.keys()):\n",
    "    l = len(dctTracks[k])\n",
    "    lengths.append(l)\n",
    "dfl = pd.DataFrame({'length': lengths, 'run': dctTracks.keys()})\n",
    "dfl.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inpArrPadded = []\n",
    "for k in dctTracks.keys():\n",
    "    aux = dctTracks[k]\n",
    "    padLength = dfl['length'].max() - len(aux)\n",
    "    auxPadded = np.pad(aux.values, ((padLength, 0), (0, 0)), mode = 'constant')\n",
    "    inpArrPadded.append(auxPadded)\n",
    "inpArrPadded = np.asarray(inpArrPadded, dtype = float)\n",
    "\n",
    "np.save(pathout/f'arr_tracksAll_batch02_ALL_lengthCorr_tides_resampled.npy', inpArrPadded, allow_pickle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test train split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augment all storms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathin = Path(r'../models/adcirc/concorde/batch02/_postprocessing/_preprocessForNN')\n",
    "## data from fort.22\n",
    "dctTracks = pd.read_pickle(pathin/'dct_tracksAll_batch02_ALL_lengthCorr_tides_resampled.pkl')\n",
    "## data from fort.63\n",
    "dctWL0 = pd.read_pickle(pathin.parent/'time_series_water_level_at_NOAA_NC_closest.pkl')\n",
    "dctWL1 = pd.read_pickle(pathin.parent/'time_series_water_level_at_NC_estuaries_closest.pkl')\n",
    "dctWL = {}\n",
    "for k in dctWL0.keys():\n",
    "    aux = dctWL1[k]\n",
    "    aux.columns = [f'zeta_pnt00{i}' for i in range(6, 9)]\n",
    "    dctWL[k] = pd.concat([dctWL0[k], aux], axis = 1)\n",
    "# ## zero padded input array\n",
    "inpArrPadded = np.load(pathin/f'arr_tracksAll_batch02_ALL_lengthCorr_tides_resampled.npy')\n",
    "\n",
    "## fort.63 of base simulation\n",
    "baseNOAA = pd.read_csv(pathin.parent/'time_series_water_level_at_NOAA_NC_closest_baseSim.csv', index_col = 0, parse_dates = True)\n",
    "baseNOAA = baseNOAA.tz_localize(None)\n",
    "baseOregon = pd.read_csv(pathin.parent/'time_series_water_level_at_NOAA_NC_closest_baseSim_newOregon.csv', index_col = 0, parse_dates = True)\n",
    "baseOregon = baseOregon.tz_localize(None)\n",
    "baseDom = pd.read_csv(pathin.parent/'time_series_water_level_at_domCenter_baseSim.csv', index_col = 0, parse_dates = True)\n",
    "baseDom = baseDom.tz_localize(None)\n",
    "baseEst = pd.read_csv(pathin.parent/'time_series_water_level_at_NC_estuaries_closest_baseSim.csv', index_col = 0, parse_dates = True)\n",
    "baseEst = baseEst.tz_localize(None)\n",
    "\n",
    "\n",
    "baseNOAA['Oregon'] = baseOregon['Oregon'].values\n",
    "baseNOAA = pd.concat([baseNOAA, baseEst], axis = 1)\n",
    "baseNOAA['Boundary'] = baseDom.values.reshape(-1)\n",
    "\n",
    "## read outputs\n",
    "dfout0 = pd.read_csv(pathin.parent/'max_water_level_at_NC_NOAA_stations.csv', index_col = 0)\n",
    "dfout1 = pd.read_csv(pathin.parent/'max_water_level_at_NC_NC_estuaries.csv', index_col = 0).sort_index()\n",
    "dfout1.index = [f'{i:04d}' for i in dfout1.index]\n",
    "dfout = pd.concat([dfout0.iloc[:-1, :], dfout1], axis = 1)\n",
    "### remove base simulation\n",
    "arrOut = np.array(dfout).reshape((dfout.shape[0], dfout.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tidal range Duck = 1.93\n",
      "Tidal range Oregon = 0.53\n",
      "Tidal range Hatteras = 0.21\n",
      "Tidal range Beaufort = 2.05\n",
      "Tidal range Wilmington = 1.78\n",
      "Tidal range Wrightsville = 2.40\n",
      "Tidal range Albemarle = 0.13\n",
      "Tidal range Pamlico = 0.19\n",
      "Tidal range Neuse = 0.17\n",
      "Tidal range Boundary = 0.73\n"
     ]
    }
   ],
   "source": [
    "dummydummy = baseNOAA.describe().T\n",
    "for i in dummydummy.index:\n",
    "    ma = dummydummy.loc[i, 'max']\n",
    "    mi = dummydummy.loc[i, 'min']\n",
    "    print(f'Tidal range {i} = {ma - mi:0.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1813it [05:06,  5.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78593, 235, 37)\n",
      "(78593, 9, 1)\n",
      "(13870, 235, 37)\n",
      "(13870, 9, 1)\n"
     ]
    }
   ],
   "source": [
    "augmented_inputs = []\n",
    "augmented_outputs = []\n",
    "nrep_per_storm = 50\n",
    "for ik, k in tqdm(enumerate(dctTracks.keys())):\n",
    "    track = dctTracks[k]\n",
    "    dfWL = dctWL[k]\n",
    "    dfWL = dfWL.tz_localize(None)\n",
    "    ## intersect timestep with storm k and remove time series at model boundary\n",
    "    dftide = baseNOAA.loc[baseNOAA.index.isin(dfWL.index), :].iloc[:, :-1]\n",
    "    dfWL.columns = dftide.columns\n",
    "    ## get only surge\n",
    "    dfSurgeOnly = dfWL - dftide\n",
    "    ## duration of storm \n",
    "    durSurge = dfSurgeOnly.index[-1] - dfSurgeOnly.index[0]\n",
    "    ## last possible random start\n",
    "    lastStart = baseNOAA.index[-1] - durSurge\n",
    "    ixLastStart = baseNOAA.index.to_list().index(lastStart)\n",
    "    ## random starting date ensuring all track time series is included in the new tide time series\n",
    "    for r in range(nrep_per_storm):\n",
    "        ixRandomStart = np.random.randint(0, ixLastStart)\n",
    "        randomStart = baseNOAA.index[ixRandomStart]\n",
    "        ## subset\n",
    "        newTide = baseNOAA.loc[randomStart:randomStart+durSurge, :].resample('1H').mean()\n",
    "        ## change dates\n",
    "        newTide.index = dfWL.index\n",
    "        ## new tide added to the surge only series after ramp\n",
    "        newWL = (newTide.iloc[:, :-1] + dfSurgeOnly).loc[track.index[0]:, :]\n",
    "        ## get max total water level\n",
    "        maxNewWL = newWL.max(axis = 0)\n",
    "\n",
    "        ## define zero padded input array as dataframe to replace new tide values\n",
    "        inpArrCp = pd.DataFrame(inpArrPadded[ik, :, :])\n",
    "        inpArrCp2 = inpArrCp.copy()\n",
    "        ## change values\n",
    "        inpArrCp2.iloc[-len(track):, -10:] = newTide.loc[newTide.index.isin(track.index), :].values\n",
    "        augmented_inputs.append(inpArrCp2.values)\n",
    "\n",
    "        augmented_outputs.append(maxNewWL.values.reshape((maxNewWL.shape[0], 1)))\n",
    "\n",
    "augmented_inputs = np.asarray(augmented_inputs)\n",
    "augmented_outputs = np.asarray(augmented_outputs)\n",
    "\n",
    "mergedInputs = np.concatenate([inpArrPadded, augmented_inputs], axis = 0)\n",
    "mergedOutputs = np.concatenate([arrOut, augmented_outputs], axis = 0)\n",
    "\n",
    "pathoutNN = Path(r'../models/NNmodel/inputs/random_split')\n",
    "\n",
    "X_train, X_test, Y_train, Y_test, idx_train, idx_test = train_test_split(mergedInputs, mergedOutputs, range(mergedInputs.shape[0]),\n",
    "                                                                         test_size=0.15, random_state=42, shuffle=True)\n",
    "\n",
    "# scaler = MinMaxScaler()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_res = X_train.reshape(X_train.shape[0] * X_train.shape[1], X_train.shape[2])\n",
    "mask = X_train_res == 0\n",
    "X_train_res[mask] = np.nan\n",
    "X_train_sc = scaler.fit_transform(X_train_res)\n",
    "X_train_sc = np.nan_to_num(X_train_sc, nan=-9999)\n",
    "X_train_sc = X_train_sc.reshape(X_train.shape)\n",
    "print(X_train_sc.shape)\n",
    "print(Y_train.shape)\n",
    "\n",
    "X_test_res = X_test.reshape(X_test.shape[0] * X_test.shape[1], X_test.shape[2])\n",
    "mask = X_test_res == 0\n",
    "X_test_res[mask] = np.nan\n",
    "X_test_sc = scaler.transform(X_test_res)\n",
    "X_test_sc = np.nan_to_num(X_test_sc, nan=-9999)\n",
    "X_test_sc = X_test_sc.reshape(X_test.shape)\n",
    "print(X_test_sc.shape)\n",
    "print(Y_test.shape)\n",
    "\n",
    "np.save(pathoutNN/f'X_train_standardScaled_allInputs_augmentedAllX{nrep_per_storm:02d}_ALL.npy', X_train_sc, allow_pickle = False)\n",
    "np.save(pathoutNN/f'y_train_augmentedAllX{nrep_per_storm:02d}_ALL.npy', Y_train, allow_pickle = False)\n",
    "np.save(pathoutNN/f'X_test_standardScaled_allInputs_augmentedAllX{nrep_per_storm:02d}_ALL.npy', X_test_sc, allow_pickle = False)\n",
    "np.save(pathoutNN/f'y_test_augmentedAllX{nrep_per_storm:02d}_ALL.npy', Y_test, allow_pickle = False)\n",
    "np.savetxt(pathoutNN/f'indices_train_standardScaled_allInputs_augmentedAllX{nrep_per_storm:02d}_ALL.txt', idx_train, fmt='%d')\n",
    "np.savetxt(pathoutNN/f'indices_test_standardScaled_allInputs_augmentedAllX{nrep_per_storm:02d}_ALL.txt', idx_test, fmt='%d')\n",
    "\n",
    "with open(pathoutNN/f'standarScaler_augmentedAllX{nrep_per_storm:02d}_ALL.pkl', 'wb') as file:\n",
    "    pickle.dump(scaler, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "concorde",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
